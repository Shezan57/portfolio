# üõ°Ô∏è Insurance Premium Prediction ‚Äì End‚Äëto‚ÄëEnd MLOps Project

A production‚Äëready, cloud‚Äënative Machine Learning system that predicts insurance (vehicle) premiums using a fully automated MLOps lifecycle: from raw data ingestion ‚Üí validation ‚Üí transformation ‚Üí model training ‚Üí evaluation with drift detection ‚Üí versioned model registry on S3 ‚Üí containerized deployment behind a CI/CD pipeline ‚Üí real‚Äëtime prediction API.

> This repository is intentionally engineered to demonstrate architectural maturity, scalability, observability, and DevOps alignment for recruiters & hiring managers.

---

## üöÄ At a Glance

| Capability | Implemented |
|------------|-------------|
| Modular Project Template (auto scaffold) | ‚úÖ |
| Reusable Local Package via `setup.py` / `pyproject.toml` | ‚úÖ |
| Data Source: MongoDB Atlas (Raw ‚Üí Data Lake) | ‚úÖ |
| Data Ingestion / Validation / Transformation Pipelines | ‚úÖ |
| Feature Engineering + EDA Notebooks | ‚úÖ |
| Custom Logger & Exception Framework | ‚úÖ |
| Model Training (Config‚ÄëDriven) | ‚úÖ |
| Model Evaluation (performance drift threshold) | ‚úÖ |
| Model Registry (Amazon S3) | ‚úÖ |
| Model Pusher (promote best model) | ‚úÖ |
| Prediction Service (`app.py`) | ‚úÖ |
| Containerization (Docker + .dockerignore) | ‚úÖ |
| CI/CD (GitHub Actions + Self‚ÄëHosted EC2 Runner) | ‚úÖ |
| Cloud Infra: AWS (IAM, S3, ECR, EC2) | ‚úÖ |
| Secure Secrets via GitHub Actions Secrets | ‚úÖ |

---

## üß© Tech Stack & Services

- **Language / Runtime:** Python 3.10  
- **Core Python Packaging:** `setup.py`, `pyproject.toml` (PEP 517/518 compliant)
- **Data Store:** MongoDB Atlas (cloud NoSQL, schema‚Äëflexible ingestion)
- **Cloud Platform:** AWS (S3 for model registry, ECR for images, EC2 for hosting & runner, IAM for security)
- **CI/CD:** GitHub Actions (with self‚Äëhosted EC2 runner for full control & private networking)
- **Containerization:** Docker (multi-stage ready structure potential)
- **Orchestration Style:** Explicit pipeline classes & artifacts (clean separation of concerns)
- **Configuration Management:** Centralized `constants/__init__.py` + typed config entities
- **Model Lifecycle:** Custom evaluation threshold & promotion logic
- **Observability:** Central logging + exception wrapping
- **Serving Layer:** Flask (via `app.py`) ‚Äì form / API endpoints (port 5080)
- **Artifact Management:** Local `artifact/` directory (git‚Äëignored) + S3 registry
- **Notebooks:** Exploratory Data Analysis & Feature Engineering
- **Security:** Environment variables for MongoDB & AWS creds (never committed)

---

## üó∫Ô∏è High-Level Architecture

```mermaid
flowchart LR
    A[MongoDB Atlas\n(Raw Collection)] --> B[Data Ingestion Component]
    B -->|Raw ‚Üí DataFrame| C[Data Validation\n(schema + checks)]
    C -->|Valid Data| D[Data Transformation\n(Feature Eng + Encoding + Splits)]
    D --> E[Model Trainer\n(ML Estimators)]
    E --> F[Model Evaluation\n(Compare vs Prod)]
    F -->|If Improved ‚â• Threshold| G[Model Pusher]
    G --> H[(S3 Model Registry\nmodel-registry/)]
    H --> I[Prediction Service\n(app.py / Docker)]
    I --> J[User / Client\n(UI / API)]
    F -->|If Not Improved| E
```

---

## üß± Project Structure (Representative)

```
MLOps-Insurance-Prediction-Full-Project/
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ aws.yaml
‚îú‚îÄ‚îÄ constants/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_validation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_transformation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_evaluation.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_pusher.py
‚îÇ   ‚îú‚îÄ‚îÄ configuration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mongo_db_connections.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aws_connection.py
‚îÇ   ‚îú‚îÄ‚îÄ data_access/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ proj1_data.py
‚îÇ   ‚îú‚îÄ‚îÄ entity/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_entity.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ artifact_entity.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ estimator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ s3_estimator.py
‚îÇ   ‚îú‚îÄ‚îÄ aws_storage/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ (S3 push/pull utilities)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ logger/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ exception/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ (other helpers‚Ä¶)
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îú‚îÄ‚îÄ mongoDB_demo.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ EDA.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.ipynb
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ (HTML Forms / Result Pages)
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ (CSS / JS / Assets)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ schema.yaml
‚îú‚îÄ‚îÄ artifact/                # (ignored: intermediate artifacts, models, transforms)
‚îî‚îÄ‚îÄ README.md
```

> NOTE: Some file names are inferred from described workflow; actual repository may differ slightly.

---

## üß™ Pipeline Components (Core Responsibilities)

| Component | Purpose | Key Output Artifact |
|-----------|---------|---------------------|
| Data Ingestion | Pull dataset from MongoDB Atlas | Raw / Ingested DataFrames |
| Data Validation | Schema + nulls + type & domain checks | Validation Report |
| Data Transformation | Feature engineering, encoding, scaling, splitting | Transformer object + train/test sets |
| Model Trainer | Train ML models (e.g., regression) | Trained Model + Metrics |
| Model Evaluation | Compare new vs production (threshold: 0.02) | Evaluation Decision |
| Model Pusher | Register improved model to S3 | Versioned Model in Registry |
| Prediction Pipeline | Load best model + transformer | Real-time predictions |

---

## üßæ Configuration & Constants

Centralized in `constants/__init__.py`:
- `MONGODB_URL` (env provided)
- AWS credentials (via env ‚Üí injected)
- `MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE = 0.02`
- `MODEL_BUCKET_NAME = "my-model-mlopsproj"`
- `MODEL_PUSHER_S3_KEY = "model-registry"`

---

## üîê Environment Variables (Never Commit Secrets)

Example (local shell):

```bash
# Mongo
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster-url/db?retryWrites=true&w=majority"

# AWS
export AWS_ACCESS_KEY_ID="YOUR_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET"
export AWS_DEFAULT_REGION="us-east-1"
```

GitHub Repository ‚Üí Settings ‚Üí Secrets and Variables ‚Üí Actions:
```
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION
ECR_REPO   # e.g., <aws_account_id>.dkr.ecr.us-east-1.amazonaws.com/vehicleproj
```

---

## üóÑÔ∏è Data Layer

- Source: MongoDB Atlas (flexible ingestion of semi-structured data)
- Ingestion converts BSON/JSON ‚Üí pandas DataFrame
- Schema constraints enforced via `config/schema.yaml`
- Validation report optionally extended for:
  - Missing ratio thresholds
  - Drift (future extension)
  - Category cardinality anomalies

---

## üß¨ Feature Engineering (Examples)

- Handling numerical vs categorical separation
- Encoding strategies (One‚ÄëHot / Ordinal / Target Encoding potential)
- Scaling: Standard / Robust (configurable)
- Train/Test split persisted with transformer object
- Reproducibility: consistent random seeds

---

## ü§ñ Model Lifecycle Strategy

1. Train candidate model with transformed datasets
2. Evaluate against existing production model (if one exists)
3. Promote only if: `new_score - prod_score >= 0.02`
4. Store promoted model & preprocessor in:
   ```
   s3://my-model-mlopsproj/model-registry/<timestamp>/
   ```
5. Prediction service always loads the latest ‚Äúactive‚Äù model

---

## üì¶ Packaging & Reusability

- `setup.py` + `pyproject.toml` allow `pip install -e .` usage
- Local imports like:
  ```python
  from src.components.data_ingestion import DataIngestion
  ```
- Encourages modular testability and potential PyPI publication.

---

## üõ†Ô∏è Local Development ‚Äì Quick Start

```bash
# 1. Clone
git clone https://github.com/Shezan57/MLOps-Insurance-Prediction-Full-Project.git
cd MLOps-Insurance-Prediction-Full-Project

# 2. Create environment
conda create -n vehicle python=3.10 -y
conda activate vehicle

# 3. Install dependencies
pip install -r requirements.txt

# 4. (Optional) Install package locally
pip install -e .

# 5. Export required environment vars (see section above)

# 6. Run training (example)
python demo.py   # or: python -m src.pipeline.training_pipeline

# 7. Launch app
python app.py
# Access: http://localhost:5080
```

Docker build & run:
```bash
docker build -t insurance-mlops .
docker run -p 5080:5080 --env-file .env insurance-mlops
```

---

## üö¢ CI/CD Workflow Highlights

| Stage | Action |
|-------|--------|
| Push / PR | Triggers GitHub Actions workflow (`aws.yaml`) |
| Build | Docker image built on self-hosted EC2 runner |
| Auth | AWS credentials from GitHub Secrets |
| Publish | Push image ‚Üí Amazon ECR |
| Deploy | Pull + run updated container on EC2 host (runner) |
| Serve | Flask app exposed on port 5080 (Security Group inbound rule) |

> Self‚ÄëHosted Runner Advantage: Full control, access to private VPC resources, cost transparency, no cold start.

---

## üóÉÔ∏è Model Registry (S3 Strategy)

- Bucket: `my-model-mlopsproj`
- Logical layout:
  ```
  model-registry/
    ‚îú‚îÄ‚îÄ 2024_12_01_15_30/
    ‚îÇ    ‚îú‚îÄ‚îÄ model.pkl
    ‚îÇ    ‚îî‚îÄ‚îÄ transformer.pkl
    ‚îî‚îÄ‚îÄ latest/  (optional symlink or pointer pattern)
  ```
- Future Enhancements: add metadata JSON (metrics, hashes, lineage)

---

## üßæ Logging & Exception Handling

- Unified logging module (timestamped, severity levels)
- Exceptions wrapped in custom class for enriched debug context
- Extendable to log aggregation tools (CloudWatch / ELK) later

---

## üåê Prediction API

Typical flow (example pseudo):
```python
from src.pipeline.prediction_pipeline import PredictionPipeline

pipeline = PredictionPipeline()
prediction = pipeline.predict(input_dict)
```

Web Interface (form submission) ‚Üí preprocess ‚Üí predict ‚Üí render result.

Training endpoint:
```
/training
```
Triggers full pipeline (for controlled retraining in controlled environments).

---

## üìä EDA & Notebooks

Located under `notebook/`:
- `mongoDB_demo.ipynb`: Data pull test from Atlas
- `EDA.ipynb`: Profiling, correlation, distributions
- `feature_engineering.ipynb`: Transformation prototyping

> Recommendation: Export finalized transformation logic into production transformer classes.

---

## üß™ Testing (Suggested Future Addition)

Recommended structure (planned roadmap):
```
tests/
  ‚îú‚îÄ‚îÄ test_data_validation.py
  ‚îú‚îÄ‚îÄ test_transformation.py
  ‚îú‚îÄ‚îÄ test_model_training.py
  ‚îî‚îÄ‚îÄ conftest.py
```
Potential frameworks: `pytest`, `great_expectations`, `pydantic` for schema.

---

## üß≠ Key Differentiators for Recruiters

- Demonstrates end-to-end ML system thinking (not just a model script)
- Cloud-native with production deployment readiness
- Clean separation between configuration, entities, components, and pipelines
- Environment & secret hygiene (no credentials in code)
- Extensible architecture (plug new models, validation rules, or storage backends)
- CI/CD integration with real infrastructure (ECR + EC2 runner)
- Emphasis on model governance (evaluation threshold + registry)

---

## üìå Roadmap / Potential Enhancements

| Priority | Enhancement |
|----------|-------------|
| ‚≠ê | Add automated unit & integration tests |
| ‚≠ê | Introduce model performance monitoring & drift dashboards |
| ‚≠ê | Add experiment tracking (MLflow / Weights & Biases) |
| ‚≠ê | Add data quality store (Great Expectations) |
| ‚≠ê | Implement async batch scoring job |
| ‚≠ê | Add feature store integration |
| ‚≠ê | Blue/Green or Shadow Deployment for new models |
| ‚≠ê | Add lineage metadata (model card JSON) |
| ‚≠ê | Terraform infrastructure as code (S3, ECR, IAM, EC2) |
| ‚≠ê | Canary rollout with health probes |
| ‚≠ê | Container security scanning (Trivy / Grype) |

---

## ‚ùì FAQ

| Question | Answer |
|----------|--------|
| Where do I change schema rules? | `config/schema.yaml` |
| How is a new model accepted? | Must exceed previous score by >= `MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE` |
| Where are artifacts stored? | Local `artifact/` (ignored) + S3 for final promoted models |
| How do I retrain? | Hit `/training` route or execute pipeline module |
| Which port is exposed? | 5080 (configure via security group + app code) |

---

## üßë‚Äçüíº Author / Contact

**Developer:** @Shezan57  
**GitHub:** [MLOps-Insurance-Prediction-Full-Project](https://github.com/Shezan57/MLOps-Insurance-Prediction-Full-Project)  
Feel free to open issues, suggest enhancements, or reach out for collaboration.

---

## üîê Disclaimer

This project is educational & demonstrative. Replace placeholder credentials, secure secrets via vaulting if used in production, and review IAM policies for least privilege before real-world deployment.

---

## ‚≠ê Support

If you find this architecture valuable:
- Star the repository
- Share with peers
- Fork and extend with your own experiments

> ‚ÄúA strong MLOps foundation multiplies the value of every model you build.‚Äù

---

### ‚úÖ Ready to Explore

Clone ‚Üí Configure env ‚Üí Run training ‚Üí Launch app ‚Üí Predict ‚Üí Inspect S3 model versions.  
Let this repository speak to your ability to ship production-grade ML systems.

Happy Building! üöÄ
